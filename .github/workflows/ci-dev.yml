name: Deploy to Dev

on:
  push:
    branches: [main]
    paths:
      # Only trigger on actual code/infrastructure changes
      - 'frontend/**'
      - 'backend/**'
      - 'terraform/deploy/**'
      - 'docker-compose.yml'
      - '.github/workflows/ci-dev.yml'  # Trigger if THIS workflow changes
  workflow_dispatch:
    inputs:
      force_full_build:
        description: "Force complete build (frontend + backend) and deploy everything"
        required: false
        default: false
        type: boolean

# Prevent deployments and destroys from running simultaneously
concurrency:
  group: terraform-dev  # Shared with destroy workflow to prevent conflicts
  cancel-in-progress: false  # Queue workflows instead of canceling

env:
  AWS_REGION: us-east-1
  ENVIRONMENT: dev

jobs:
  # Code Quality Checks - Run first, fail fast if code doesn't meet standards

  # Check which paths changed to conditionally run quality checks and deployments
  check-changed-paths:
    runs-on: ubuntu-latest
    outputs:
      frontend: ${{ steps.filter.outputs.frontend }}
      backend: ${{ steps.filter.outputs.backend }}
      infrastructure: ${{ steps.filter.outputs.infrastructure }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            frontend:
              - 'frontend/**'
              - 'packages/**'
              - '.github/workflows/ci-dev.yml'
            backend:
              - 'backend/**'
              - 'packages/**'
              - 'Dockerfile.*'
              - '.github/workflows/ci-dev.yml'
            infrastructure:
              - 'terraform/deploy/**'
              - '.github/workflows/ci-dev.yml'

  # Detect whether the previous run on this branch had any failures
  check-previous-run:
    runs-on: ubuntu-latest
    outputs:
      prev_backend_failed: ${{ steps.out.outputs.prev_backend_failed }}
      prev_backend_deploy_failed: ${{ steps.out.outputs.prev_backend_deploy_failed }}
      prev_frontend_failed: ${{ steps.out.outputs.prev_frontend_failed }}
      prev_build_failed: ${{ steps.out.outputs.prev_build_failed }}
      prev_infra_failed: ${{ steps.out.outputs.prev_infra_failed }}
      prev_overall_failed: ${{ steps.out.outputs.prev_overall_failed }}
    steps:
      - name: Inspect previous workflow run for failures
        id: out
        uses: actions/github-script@v7
        with:
          script: |
            const branch = context.ref.replace('refs/heads/', '');
            const { data: runs } = await github.rest.actions.listWorkflowRunsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              branch,
              per_page: 10,
              status: 'completed'
            });
            // Find last completed run before this one
            const prev = runs.workflow_runs.find(r => r.id < context.runId && r.name === context.workflow);
            let backendFailed = false;
            let backendDeployFailed = false;
            let frontendFailed = false;
            let buildFailed = false;
            let infraFailed = false;

            if (prev) {
              const { data: jobs } = await github.rest.actions.listJobsForWorkflowRun({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: prev.id
              });
              for (const j of jobs.jobs) {
                if (j.conclusion === 'failure') {
                  console.log(`âŒ Previous run had failed job: ${j.name}`);
                  // Specific job failures
                  if (j.name === 'backend-quality') backendFailed = true;
                  if (j.name === 'deploy-services') backendDeployFailed = true;
                  if (j.name === 'frontend-quality') frontendFailed = true;
                  if (j.name === 'build-and-push') buildFailed = true;
                  if (j.name === 'deploy-infrastructure') infraFailed = true;
                }
              }
            }

            // Overall failure if ANY job failed
            const anyFailure = backendFailed || backendDeployFailed || frontendFailed || buildFailed || infraFailed;

            core.setOutput('prev_backend_failed', backendFailed ? 'true' : 'false');
            core.setOutput('prev_backend_deploy_failed', backendDeployFailed ? 'true' : 'false');
            core.setOutput('prev_frontend_failed', frontendFailed ? 'true' : 'false');
            core.setOutput('prev_build_failed', buildFailed ? 'true' : 'false');
            core.setOutput('prev_infra_failed', infraFailed ? 'true' : 'false');
            core.setOutput('prev_overall_failed', anyFailure ? 'true' : 'false');

            if (anyFailure) {
              console.log('ğŸ”„ Previous run had failures - will run all steps in this run');
            } else {
              console.log('âœ… Previous run was successful');
            }

  # Check for last successful complete build (frontend + backend)
  check-last-complete-build:
    runs-on: ubuntu-latest
    outputs:
      last_complete_build_sha: ${{ steps.find.outputs.last_complete_build_sha }}
      needs_complete_build: ${{ steps.find.outputs.needs_complete_build }}
    steps:
      - name: Find last successful complete build
        id: find
        uses: actions/github-script@v7
        with:
          script: |
            const branch = context.ref.replace('refs/heads/', '');
            const { data: runs } = await github.rest.actions.listWorkflowRunsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              branch,
              per_page: 20,
              status: 'completed'
            });

            // Find last successful run where build-and-push job succeeded
            // and it was a complete build (both frontend and backend)
            let lastCompleteBuildSha = null;
            let needsCompleteBuild = false;

            for (const run of runs.workflow_runs) {
              if (run.id >= context.runId) continue; // Skip current and future runs
              if (run.conclusion !== 'success') continue; // Only successful runs
              if (run.name !== context.workflow) continue; // Same workflow

              const { data: jobs } = await github.rest.actions.listJobsForWorkflowRun({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: run.id
              });

              // Check if build-and-push succeeded
              const buildJob = jobs.jobs.find(j => j.name === 'build-and-push');
              if (!buildJob || buildJob.conclusion !== 'success') continue;

              // For now, if build-and-push succeeded, we assume it was a complete build
              // (In the old workflow, build-and-push always built all 4 images)
              // Going forward, we'll check job outputs to verify completeness
              try {
                const buildJobDetails = await github.rest.actions.getJobForWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  job_id: buildJob.id
                });

                // Check if all 4 build steps completed successfully
                // Steps: Build and push Identity Service, API Gateway, Data Service, Frontend
                const buildSteps = buildJobDetails.data.steps.filter(s =>
                  s.name.includes('Build and push') && s.conclusion === 'success'
                );

                // Complete build should have at least 4 build steps (3 backend + 1 frontend)
                // If we can't verify, assume it was complete if build-and-push succeeded
                // (backward compatibility with old workflow that always built everything)
                if (buildSteps.length >= 4 || buildSteps.length === 0) {
                  lastCompleteBuildSha = run.head_sha;
                  console.log(`âœ… Found last complete build at commit: ${lastCompleteBuildSha.substring(0, 7)}`);
                  if (buildSteps.length === 0) {
                    console.log('   (Assuming complete build - step details unavailable)');
                  }
                  break;
                }
              } catch (error) {
                // If we can't get job details, assume it was a complete build if build-and-push succeeded
                // This is backward compatible with the old workflow
                console.log(`âš ï¸  Could not verify build completeness for run ${run.id}, assuming complete (backward compatibility)`);
                lastCompleteBuildSha = run.head_sha;
                console.log(`âœ… Found last complete build at commit: ${lastCompleteBuildSha.substring(0, 7)}`);
                break;
              }
            }

            if (!lastCompleteBuildSha) {
              console.log('âš ï¸  No previous complete build found - will do full build');
              needsCompleteBuild = true;
            } else {
              // Check if there were any commits between last complete build and current commit
              // that might have had build failures
              const { data: commits } = await github.rest.repos.compareCommits({
                owner: context.repo.owner,
                repo: context.repo.repo,
                base: lastCompleteBuildSha,
                head: context.sha
              });

              // Check if any intermediate runs had build failures
              for (const run of runs.workflow_runs) {
                if (run.id >= context.runId) continue;
                if (run.head_sha === lastCompleteBuildSha) break; // Stop at last complete build

                // Check if this intermediate run had a build failure
                if (run.conclusion === 'failure' || run.conclusion === 'cancelled') {
                  const { data: jobs } = await github.rest.actions.listJobsForWorkflowRun({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    run_id: run.id
                  });

                  const buildJob = jobs.jobs.find(j => j.name === 'build-and-push');
                  if (buildJob && buildJob.conclusion === 'failure') {
                    console.log(`âš ï¸  Found build failure between last complete build and current commit - will do full build`);
                    needsCompleteBuild = true;
                    break;
                  }
                }
              }

              console.log(`ğŸ“Š Last complete build: ${lastCompleteBuildSha.substring(0, 7)}`);
              console.log(`ğŸ“Š Current commit: ${context.sha.substring(0, 7)}`);
              console.log(`ğŸ“Š Commits in between: ${commits.behind_by || 0}`);
            }

            core.setOutput('last_complete_build_sha', lastCompleteBuildSha || '');
            core.setOutput('needs_complete_build', needsCompleteBuild ? 'true' : 'false');

  frontend-quality:
    runs-on: ubuntu-latest
    needs: [check-changed-paths, check-previous-run]
    # Only run if frontend, shared packages, or workflow changed
    # OR if previous run had any failures (to ensure everything runs after a failed run)
    # OR if force_full_build is true
    if: needs.check-changed-paths.outputs.frontend == 'true' || needs.check-previous-run.outputs.prev_overall_failed == 'true' || (github.event_name == 'workflow_dispatch' && inputs.force_full_build == true)
    defaults:
      run:
        working-directory: ./frontend
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json

      - name: Build unit-conversion package
        run: |
          cd ../packages/unit-conversion/typescript
          npm ci
          npm run build

      - name: Install dependencies
        run: npm ci

      - name: Run ESLint
        run: npm run lint

      - name: Fail if forbidden localhost URLs are present
        run: |
          echo "Scanning for forbidden localhost/127.0.0.1 URLs..."
          if rg -n "http://localhost|https://localhost|127.0.0.1" src/; then
            echo "âŒ Forbidden localhost URL(s) found in source. Use runtime config/api client instead.";
            exit 1;
          else
            echo "âœ… No forbidden URLs found";
          fi

      - name: Run TypeScript check
        run: npm run type-check

      - name: Run Prettier check
        run: npm run format-check

  backend-quality:
    runs-on: ubuntu-latest
    needs: [check-changed-paths, check-previous-run]
    # Only run if backend, shared packages, or workflow changed
    # OR if previous run had any failures (to ensure everything runs after a failed run)
    # OR if force_full_build is true
    if: needs.check-changed-paths.outputs.backend == 'true' || needs.check-previous-run.outputs.prev_overall_failed == 'true' || (github.event_name == 'workflow_dispatch' && inputs.force_full_build == true)
    defaults:
      run:
        working-directory: ./backend
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0.x"

      - name: Restore dependencies
        run: dotnet restore

      - name: Run dotnet format check
        run: dotnet format --verify-no-changes --verbosity diagnostic

      - name: Build solution
        run: dotnet build --no-restore

      - name: Run tests
        run: dotnet test --no-build --verbosity normal

  check-infrastructure:
    runs-on: ubuntu-latest
    needs: [check-changed-paths, check-previous-run, frontend-quality, backend-quality]
    # Run even if quality checks were skipped (but not if they failed)
    if: |
      always() &&
      (needs.frontend-quality.result == 'success' || needs.frontend-quality.result == 'skipped') &&
      (needs.backend-quality.result == 'success' || needs.backend-quality.result == 'skipped')
    outputs:
      has-secrets: ${{ steps.check.outputs.has-secrets }}
    steps:
      - name: Check if AWS infrastructure is configured
        id: check
        run: |
          if [ -z "${{ secrets.ECR_IDENTITY_SERVICE_URL }}" ]; then
            echo "âš ï¸ AWS infrastructure not yet deployed"
            echo "ğŸ“‹ To enable deployments:"
            echo "   1. Run: cd terraform/setup && terraform init && terraform apply"
            echo "   2. Configure GitHub secrets (see docs/GITHUB_SECRETS.md)"
            echo "   3. Deployments will run automatically on next push"
            echo "has-secrets=false" >> $GITHUB_OUTPUT
          else
            echo "âœ… AWS infrastructure configured - proceeding with deployment"
            echo "has-secrets=true" >> $GITHUB_OUTPUT
          fi

  build-and-push:
    runs-on: ubuntu-latest
    needs: [frontend-quality, backend-quality, check-infrastructure, check-previous-run, check-last-complete-build, check-changed-paths]
    # Only run if AWS infrastructure is configured
    if: needs.check-infrastructure.outputs.has-secrets == 'true'
    outputs:
      identity-image: ${{ steps.build-identity.outputs.image || '' }}
      gateway-image: ${{ steps.build-gateway.outputs.image || '' }}
      data-image: ${{ steps.build-data.outputs.image || '' }}
      frontend-image: ${{ steps.build-frontend.outputs.image || '' }}
      built-backend: ${{ steps.build-identity.outcome == 'success' || steps.build-gateway.outcome == 'success' || steps.build-data.outcome == 'success' }}
      built-frontend: ${{ steps.build-frontend.outcome == 'success' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Determine what to build
        id: build-plan
        run: |
          # Determine if we need to build backend
          BUILD_BACKEND=false
          if [ "${{ needs.check-changed-paths.outputs.backend }}" == "true" ]; then
            BUILD_BACKEND=true
            echo "âœ… Backend paths changed - will build backend"
          elif [ "${{ needs.check-previous-run.outputs.prev_overall_failed }}" == "true" ]; then
            BUILD_BACKEND=true
            echo "âœ… Previous run failed - will build backend"
          elif [ "${{ needs.check-last-complete-build.outputs.needs_complete_build }}" == "true" ]; then
            BUILD_BACKEND=true
            echo "âœ… No complete build found or failures detected - will build backend"
          elif [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ "${{ inputs.force_full_build }}" == "true" ]; then
            BUILD_BACKEND=true
            echo "âœ… Force full build requested - will build backend"
          fi

          # Determine if we need to build frontend
          BUILD_FRONTEND=false
          if [ "${{ needs.check-changed-paths.outputs.frontend }}" == "true" ]; then
            BUILD_FRONTEND=true
            echo "âœ… Frontend paths changed - will build frontend"
          elif [ "${{ needs.check-previous-run.outputs.prev_overall_failed }}" == "true" ]; then
            BUILD_FRONTEND=true
            echo "âœ… Previous run failed - will build frontend"
          elif [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ "${{ inputs.force_full_build }}" == "true" ]; then
            BUILD_FRONTEND=true
            echo "âœ… Force full build requested - will build frontend"
          fi

          # If frontend-only change but no complete build exists, need to build backend too
          if [ "$BUILD_FRONTEND" == "true" ] && [ "$BUILD_BACKEND" == "false" ]; then
            if [ "${{ needs.check-last-complete-build.outputs.needs_complete_build }}" == "true" ]; then
              BUILD_BACKEND=true
              echo "âš ï¸  Frontend-only change but no complete build found - will build backend too"
            fi
          fi

          echo "build_backend=$BUILD_BACKEND" >> $GITHUB_OUTPUT
          echo "build_frontend=$BUILD_FRONTEND" >> $GITHUB_OUTPUT

          echo ""
          echo "ğŸ“‹ Build Plan:"
          echo "  Backend:  $BUILD_BACKEND"
          echo "  Frontend: $BUILD_FRONTEND"

      - name: Build and push Identity Service
        id: build-identity
        if: steps.build-plan.outputs.build_backend == 'true'
        run: |
          IMAGE_TAG="${GITHUB_SHA:0:7}"
          docker build -f backend/IdentityService/Dockerfile -t ${{ secrets.ECR_IDENTITY_SERVICE_URL }}:$IMAGE_TAG -t ${{ secrets.ECR_IDENTITY_SERVICE_URL }}:latest .
          docker push ${{ secrets.ECR_IDENTITY_SERVICE_URL }}:$IMAGE_TAG
          docker push ${{ secrets.ECR_IDENTITY_SERVICE_URL }}:latest
          echo "image=${{ secrets.ECR_IDENTITY_SERVICE_URL }}:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Build and push API Gateway
        id: build-gateway
        if: steps.build-plan.outputs.build_backend == 'true'
        run: |
          IMAGE_TAG="${GITHUB_SHA:0:7}"
          docker build -f backend/ApiGateway/Dockerfile -t ${{ secrets.ECR_API_GATEWAY_URL }}:$IMAGE_TAG -t ${{ secrets.ECR_API_GATEWAY_URL }}:latest .
          docker push ${{ secrets.ECR_API_GATEWAY_URL }}:$IMAGE_TAG
          docker push ${{ secrets.ECR_API_GATEWAY_URL }}:latest
          echo "image=${{ secrets.ECR_API_GATEWAY_URL }}:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Build and push Data Service
        id: build-data
        if: steps.build-plan.outputs.build_backend == 'true'
        run: |
          IMAGE_TAG="${GITHUB_SHA:0:7}"
          docker build -f backend/DataService/Dockerfile -t ${{ secrets.ECR_DATA_SERVICE_URL }}:$IMAGE_TAG -t ${{ secrets.ECR_DATA_SERVICE_URL }}:latest .
          docker push ${{ secrets.ECR_DATA_SERVICE_URL }}:$IMAGE_TAG
          docker push ${{ secrets.ECR_DATA_SERVICE_URL }}:latest
          echo "image=${{ secrets.ECR_DATA_SERVICE_URL }}:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Build and push Frontend
        id: build-frontend
        if: steps.build-plan.outputs.build_frontend == 'true'
        run: |
          IMAGE_TAG="${GITHUB_SHA:0:7}"
          docker build -f frontend/Dockerfile -t ${{ secrets.ECR_FRONTEND_URL }}:$IMAGE_TAG -t ${{ secrets.ECR_FRONTEND_URL }}:latest .
          docker push ${{ secrets.ECR_FRONTEND_URL }}:$IMAGE_TAG
          docker push ${{ secrets.ECR_FRONTEND_URL }}:latest
          echo "image=${{ secrets.ECR_FRONTEND_URL }}:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Summary
        if: always()
        run: |
          echo "ğŸ“Š Build Summary:"
          echo "  Identity Service: ${{ steps.build-identity.outcome }}"
          echo "  API Gateway:     ${{ steps.build-gateway.outcome }}"
          echo "  Data Service:     ${{ steps.build-data.outcome }}"
          echo "  Frontend:         ${{ steps.build-frontend.outcome }}"

  deploy-infrastructure:
    runs-on: ubuntu-latest
    needs: [check-infrastructure, build-and-push, check-changed-paths, check-previous-run]
    # Only run if build-and-push succeeded AND infrastructure changed (or previous run failed)
    if: needs.check-infrastructure.outputs.has-secrets == 'true' && needs.build-and-push.result == 'success' && (
          needs.check-changed-paths.outputs.infrastructure == 'true' ||
          needs.check-previous-run.outputs.prev_overall_failed == 'true' ||
          (github.event_name == 'workflow_dispatch' && inputs.force_full_build == true)
        )
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.9.0"

      - name: Create backend config
        run: |
          cat > backend-config.tfvars <<EOF
          bucket         = "${{ secrets.S3_BUCKET_NAME }}"
          key            = "deploy/dev/terraform.tfstate"
          region         = "${{ env.AWS_REGION }}"
          dynamodb_table = "${{ secrets.DYNAMODB_TABLE_NAME }}"
          EOF
        working-directory: ./terraform/deploy

      # NOTE: No longer creating terraform.tfvars!
      # All infrastructure values (VPC, ECR, Cognito, etc.) are now automatically
      # pulled from terraform_remote_state in setup. Only environment-specific
      # variables are in environments/dev.tfvars.

      - name: Check if backend resources exist
        id: check-backend
        run: |
          # Check if S3 bucket exists
          if ! aws s3 ls s3://${{ secrets.S3_BUCKET_NAME }} 2>/dev/null; then
            echo "âŒ ERROR: S3 bucket '${{ secrets.S3_BUCKET_NAME }}' does not exist!"
            echo "Please run Phase 4 (AWS Infrastructure Setup) first:"
            echo "  cd terraform/setup"
            echo "  terraform init"
            echo "  terraform apply"
            exit 1
          fi

          # Check if DynamoDB table exists
          if ! aws dynamodb describe-table --table-name ${{ secrets.DYNAMODB_TABLE_NAME }} 2>/dev/null; then
            echo "âŒ ERROR: DynamoDB table '${{ secrets.DYNAMODB_TABLE_NAME }}' does not exist!"
            echo "Please run Phase 4 (AWS Infrastructure Setup) first:"
            echo "  cd terraform/setup"
            echo "  terraform init"
            echo "  terraform apply"
            exit 1
          fi

          echo "âœ… Backend resources verified"

      - name: Terraform Init
        run: terraform init -backend-config=backend-config.tfvars
        working-directory: ./terraform/deploy

      - name: Terraform Plan
        run: terraform plan -var-file="environments/dev.tfvars" -out=tfplan
        working-directory: ./terraform/deploy

      - name: Terraform Apply
        run: |
          echo "ğŸš€ Applying Terraform configuration..."
          terraform apply -auto-approve tfplan

          echo ""
          echo "ğŸ“‹ Checking if S3/CloudFront module resources were created..."
          terraform state list | grep "module.s3_cloudfront" || echo "âš ï¸ No S3/CloudFront resources in state!"
        working-directory: ./terraform/deploy

      - name: Get CloudFront domain for CORS
        id: get-cloudfront
        run: |
          echo "ğŸ“Š Getting CloudFront domain..."
          CLOUDFRONT_DOMAIN=$(terraform output -raw cloudfront_domain_name || echo "")
          echo "CloudFront Domain: $CLOUDFRONT_DOMAIN"
          echo "cloudfront_domain=$CLOUDFRONT_DOMAIN" >> $GITHUB_OUTPUT
        working-directory: ./terraform/deploy

      - name: Update API Gateway CORS (Second Apply)
        if: steps.get-cloudfront.outputs.cloudfront_domain != ''
        run: |
          echo "ğŸ”„ Updating API Gateway with CORS configuration..."
          echo "CloudFront Domain: ${{ steps.get-cloudfront.outputs.cloudfront_domain }}"

          # Second apply to update API Gateway with CloudFront domain for CORS
          terraform apply -auto-approve \
            -var-file="environments/dev.tfvars" \
            -var="cloudfront_domain_override=${{ steps.get-cloudfront.outputs.cloudfront_domain }}"

          echo "âœ… API Gateway CORS configured!"
        working-directory: ./terraform/deploy

      - name: Get outputs
        id: tf-outputs
        run: |
          echo "ğŸ“Š Getting Terraform outputs..."

          # Show all outputs for debugging
          echo "All outputs:"
          terraform output

          # Get individual outputs with explicit logging
          echo ""
          echo "Capturing outputs to GITHUB_OUTPUT..."

          # Get outputs using environment variables as intermediate (avoids GitHub Actions metadata)
          export TF_RDS=$(terraform output -raw rds_endpoint)
          export TF_ID_SVC=$(terraform output -raw identity_service_url)
          export TF_API_GW=$(terraform output -raw api_gateway_url)
          export TF_DATA_SVC=$(terraform output -raw data_service_url)
          export TF_CF_DOMAIN=$(terraform output -raw cloudfront_domain_name)
          export TF_S3_BUCKET=$(terraform output -raw frontend_s3_bucket_name)

          echo "Captured values:"
          echo "  rds_endpoint: '$TF_RDS'"
          echo "  identity_service_url: '$TF_ID_SVC'"
          echo "  api_gateway_url: '$TF_API_GW'"
          echo "  data_service_url: '$TF_DATA_SVC'"
          echo "  cloudfront_domain: '$TF_CF_DOMAIN'"
          echo "  s3_bucket: '$TF_S3_BUCKET'"

          # Write to GITHUB_OUTPUT using printf (more reliable than echo)
          printf "rds_endpoint=%s\n" "$TF_RDS" >> "$GITHUB_OUTPUT"
          printf "identity_service_url=%s\n" "$TF_ID_SVC" >> "$GITHUB_OUTPUT"
          printf "api_gateway_url=%s\n" "$TF_API_GW" >> "$GITHUB_OUTPUT"
          printf "data_service_url=%s\n" "$TF_DATA_SVC" >> "$GITHUB_OUTPUT"
          printf "cloudfront_domain=%s\n" "$TF_CF_DOMAIN" >> "$GITHUB_OUTPUT"
          printf "s3_bucket=%s\n" "$TF_S3_BUCKET" >> "$GITHUB_OUTPUT"

          echo ""
          echo "âœ… All outputs captured successfully"
        working-directory: ./terraform/deploy

    outputs:
      rds_endpoint: ${{ steps.tf-outputs.outputs.rds_endpoint }}
      identity_service_url: ${{ steps.tf-outputs.outputs.identity_service_url }}
      api_gateway_url: ${{ steps.tf-outputs.outputs.api_gateway_url }}
      data_service_url: ${{ steps.tf-outputs.outputs.data_service_url }}
      cloudfront_domain: ${{ steps.tf-outputs.outputs.cloudfront_domain }}
      s3_bucket: ${{ steps.tf-outputs.outputs.s3_bucket }}

  deploy-services:
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, build-and-push, check-changed-paths, check-previous-run]
    if: needs.build-and-push.result == 'success' && (
          needs.check-changed-paths.outputs.backend == 'true' ||
          needs.check-previous-run.outputs.prev_backend_failed == 'true' ||
          needs.check-previous-run.outputs.prev_backend_deploy_failed == 'true' ||
          needs.check-previous-run.outputs.prev_overall_failed == 'true' ||
          needs.build-and-push.outputs.built-backend == 'true' ||
          (github.event_name == 'workflow_dispatch' && inputs.force_full_build == true)
        )

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Trigger App Runner service deployments
        run: |
          echo "ğŸš€ Triggering App Runner service deployments..."
          echo "This will pull the latest :latest tagged images from ECR"
          echo ""

          # Get service ARNs
          IDENTITY_ARN=$(aws apprunner list-services \
            --query "ServiceSummaryList[?contains(ServiceName, '${{ env.ENVIRONMENT }}-identity-service')].ServiceArn | [0]" \
            --output text)

          DATA_ARN=$(aws apprunner list-services \
            --query "ServiceSummaryList[?contains(ServiceName, '${{ env.ENVIRONMENT }}-data-service')].ServiceArn | [0]" \
            --output text)

          API_ARN=$(aws apprunner list-services \
            --query "ServiceSummaryList[?contains(ServiceName, '${{ env.ENVIRONMENT }}-api-gateway')].ServiceArn | [0]" \
            --output text)

          # Trigger deployments
          echo "ğŸ“¦ Deploying Identity Service..."
          if [ -n "$IDENTITY_ARN" ] && [ "$IDENTITY_ARN" != "None" ]; then
            aws apprunner start-deployment --service-arn "$IDENTITY_ARN"
            echo "âœ… Identity Service deployment initiated"
          else
            echo "âš ï¸  Identity Service not found, skipping"
          fi

          echo ""
          echo "ğŸ“¦ Deploying Data Service..."
          if [ -n "$DATA_ARN" ] && [ "$DATA_ARN" != "None" ]; then
            aws apprunner start-deployment --service-arn "$DATA_ARN"
            echo "âœ… Data Service deployment initiated"
          else
            echo "âš ï¸  Data Service not found, skipping"
          fi

          echo ""
          echo "â­ï¸  Skipping API Gateway deployment"
          echo "   (Already deployed by update-cors job)"

          echo ""
          echo "â³ Note: Deployments take ~3-5 minutes to complete"
          echo "   Services will automatically apply database migrations on startup"
          echo "   Monitor logs: aws logs tail /aws/apprunner/navarch-studio-${{ env.ENVIRONMENT }}-data-service --since 5m --region ${{ env.AWS_REGION }}"

      - name: Wait for services to start
        run: |
          echo "â³ Waiting for services to start and apply migrations..."
          echo "   App Runner deployments typically take 3-5 minutes"
          sleep 180  # Wait 3 minutes for services to start

      - name: Verify migrations applied
        run: |
          echo "ğŸ” Verifying database migrations were applied successfully..."

          # Log group names for services
          IDENTITY_LOG_GROUP="/aws/apprunner/navarch-studio-${{ env.ENVIRONMENT }}-identity-service"
          DATA_LOG_GROUP="/aws/apprunner/navarch-studio-${{ env.ENVIRONMENT }}-data-service"

          # Calculate start time (15 minutes ago to catch recent deployments)
          START_TIME=$(($(date +%s) - 900))000

          # Check Identity Service migrations
          echo ""
          echo "Checking Identity Service migrations..."

          # Identity Service uses emoji prefixes: "âœ… Migrations applied successfully!" or "âœ… Database schema is up to date"
          IDENTITY_MIGRATION_SUCCESS=$(aws logs filter-log-events \
            --log-group-name "$IDENTITY_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "Migrations applied successfully" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          IDENTITY_MIGRATION_UP_TO_DATE=$(aws logs filter-log-events \
            --log-group-name "$IDENTITY_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "Database schema is up to date" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          if [ -n "$IDENTITY_MIGRATION_SUCCESS" ] || [ -n "$IDENTITY_MIGRATION_UP_TO_DATE" ]; then
            echo "âœ… Identity Service: Migrations verified"
          else
            echo "âš ï¸  Identity Service: Migration status inconclusive (may still be starting)"
            echo "   Check logs manually: aws logs tail $IDENTITY_LOG_GROUP --since 15m --region ${{ env.AWS_REGION }}"
            echo "   Looking for: 'âœ… Migrations applied successfully!' or 'âœ… Database schema is up to date'"
          fi

          # Check Data Service migrations
          echo ""
          echo "Checking Data Service migrations..."

          # Data Service uses [MIGRATION] prefix: "[MIGRATION] Migrations applied successfully!" or "[MIGRATION] Database schema is up to date"
          DATA_MIGRATION_SUCCESS=$(aws logs filter-log-events \
            --log-group-name "$DATA_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "[MIGRATION] Migrations applied successfully" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          DATA_MIGRATION_UP_TO_DATE=$(aws logs filter-log-events \
            --log-group-name "$DATA_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "[MIGRATION] Database schema is up to date" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          if [ -n "$DATA_MIGRATION_SUCCESS" ] || [ -n "$DATA_MIGRATION_UP_TO_DATE" ]; then
            echo "âœ… Data Service: Migrations verified"
          else
            echo "âš ï¸  Data Service: Migration status inconclusive (may still be starting)"
            echo "   Check logs manually: aws logs tail $DATA_LOG_GROUP --since 15m --region ${{ env.AWS_REGION }}"
            echo "   Looking for: '[MIGRATION] Migrations applied successfully!' or '[MIGRATION] Database schema is up to date'"
          fi

          # Check for migration errors (fail if found)
          echo ""
          echo "Checking for migration errors..."
          IDENTITY_ERROR=$(aws logs filter-log-events \
            --log-group-name "$IDENTITY_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "Migration check failed" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          DATA_ERROR=$(aws logs filter-log-events \
            --log-group-name "$DATA_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "Migration check failed" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          if [ -n "$IDENTITY_ERROR" ]; then
            echo "âŒ Identity Service migration failed!"
            echo "$IDENTITY_ERROR"
            exit 1
          fi

          if [ -n "$DATA_ERROR" ]; then
            echo "âŒ Data Service migration failed!"
            echo "$DATA_ERROR"
            exit 1
          fi

          # Check template vessel seeding (Data Service only)
          echo ""
          echo "Checking template vessel seeding..."
          TEMPLATE_SEED_SUCCESS=$(aws logs filter-log-events \
            --log-group-name "$DATA_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "[SEED] Template vessel seeding completed" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          TEMPLATE_SEED_EXISTS=$(aws logs filter-log-events \
            --log-group-name "$DATA_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "Template vessel.*already exists" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          TEMPLATE_SEED_RESTORED=$(aws logs filter-log-events \
            --log-group-name "$DATA_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "Restored existing template vessel" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          TEMPLATE_SEED_SUCCESS_FULL=$(aws logs filter-log-events \
            --log-group-name "$DATA_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "Successfully seeded template vessel" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          if [ -n "$TEMPLATE_SEED_SUCCESS" ] || [ -n "$TEMPLATE_SEED_EXISTS" ] || [ -n "$TEMPLATE_SEED_RESTORED" ] || [ -n "$TEMPLATE_SEED_SUCCESS_FULL" ]; then
            echo "âœ… Template vessel seeding verified"
          else
            echo "âš ï¸  Template vessel seeding status inconclusive"
            echo "   Check logs manually: aws logs tail $DATA_LOG_GROUP --since 15m --region ${{ env.AWS_REGION }}"
            echo "   Looking for: '[SEED] Template vessel seeding completed' or 'Successfully seeded template vessel'"
            echo ""
            echo "   Note: This may be normal if the template vessel already exists from a previous deployment."
          fi

          # Check for template vessel seeding errors
          TEMPLATE_SEED_ERROR=$(aws logs filter-log-events \
            --log-group-name "$DATA_LOG_GROUP" \
            --start-time "$START_TIME" \
            --filter-pattern "[SEED] WARNING: Failed to seed template vessel" \
            --query 'events[*].message' \
            --output text 2>/dev/null || echo "")

          if [ -n "$TEMPLATE_SEED_ERROR" ]; then
            echo "âš ï¸  Template vessel seeding had warnings (non-fatal):"
            echo "$TEMPLATE_SEED_ERROR"
            echo ""
            echo "   Note: Template vessel seeding is optional and will not block deployment."
          fi

          echo ""
          echo "âœ… Migration verification complete"
          echo "   Both services have either applied migrations successfully or are up to date"

  deploy-frontend:
    runs-on: ubuntu-latest
    # Depend on infrastructure deployment and build-and-push
    needs:
      - check-infrastructure
      - check-changed-paths
      - check-previous-run
      - build-and-push
      - deploy-infrastructure
      - deploy-services
    # Run when infra is configured and frontend changed, or via manual overrides
    # OR if previous run had any failures (to ensure everything runs after a failed run)
    # Always wait for deploy-infrastructure and deploy-services (will be skipped if not needed)
    # Note: deploy-services may be skipped if backend wasn't built, but that's OK - services should already exist
    if: |
      needs.check-infrastructure.outputs.has-secrets == 'true' &&
      needs.build-and-push.result == 'success' &&
      (needs.deploy-infrastructure.result == 'success' || needs.deploy-infrastructure.result == 'skipped') &&
      (needs.deploy-services.result == 'success' || needs.deploy-services.result == 'skipped') &&
      (
        needs.check-changed-paths.outputs.frontend == 'true' ||
        needs.check-previous-run.outputs.prev_overall_failed == 'true' ||
        needs.build-and-push.outputs.built-frontend == 'true' ||
        (github.event_name == 'workflow_dispatch' && inputs.force_full_build == true)
      )

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Discover S3 bucket name from AWS
        id: discover-s3
        run: |
          echo "ğŸ” Querying AWS for frontend S3 bucket..."
          # List buckets and filter for the pattern: *-dev-frontend
          BUCKET_NAME=$(aws s3api list-buckets --query "Buckets[?contains(Name, '-dev-frontend')].Name | [0]" --output text)

          if [ -z "$BUCKET_NAME" ] || [ "$BUCKET_NAME" == "None" ]; then
            echo "âŒ ERROR: Could not find S3 bucket matching pattern '*-dev-frontend'"
            exit 1
          fi

          echo "âœ… Found S3 bucket: $BUCKET_NAME"
          echo "bucket_name=$BUCKET_NAME" >> $GITHUB_OUTPUT

      - name: Discover API Gateway URL from App Runner
        id: discover-api
        run: |
          echo "ğŸ” Discovering API Gateway service URL from App Runner..."
          API_GW=$(aws apprunner list-services --query "ServiceSummaryList[?contains(ServiceName, '-dev-api-gateway')].ServiceUrl | [0]" --output text)
          if [ -z "$API_GW" ] || [ "$API_GW" == "None" ]; then
            echo "âŒ ERROR: API Gateway not found!"
            echo "âš ï¸  This may indicate that backend services need to be deployed first."
            echo "   If you're doing a frontend-only deployment, ensure backend was built and deployed in a previous run."
            echo "   Otherwise, trigger a full build with force_full_build=true"
            exit 1
          else
            # Compose full https URL
            FULL_API_URL="https://$API_GW"
            echo "API Gateway ServiceUrl: $API_GW"
            echo "API_GATEWAY_URL=$FULL_API_URL" >> $GITHUB_ENV
            echo "âœ… API Gateway URL set to $FULL_API_URL"
          fi

      - name: Health check API Gateway (retry)
        run: |
          echo "ğŸ” Checking API health at $API_GATEWAY_URL/health ..."
          for i in {1..12}; do
            if curl -sf "$API_GATEWAY_URL/health" > /dev/null; then
              echo "âœ… API healthy"
              exit 0
            fi
            echo "â³ API not healthy yet, retry $i/12..."
            sleep 5
          done
          echo "âš ï¸  API not healthy after retries, but continuing with frontend deployment"
          echo "   The frontend will be built with the API Gateway URL, but services may still be starting"

      - name: Check required outputs
        run: |
          echo "ğŸ” Checking deployment configuration..."
          echo "S3 Bucket: '${{ steps.discover-s3.outputs.bucket_name }}'"
          echo "API Gateway URL: '$API_GATEWAY_URL'"

          if [ -z "$API_GATEWAY_URL" ]; then
            echo "âŒ ERROR: API Gateway URL is empty! Cannot deploy frontend."
            exit 1
          fi

          echo "âœ… All required configuration available"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json

      - name: Build unit-conversion package
        run: |
          cd packages/unit-conversion/typescript
          npm ci
          npm run build

      - name: Install dependencies
        run: npm ci
        working-directory: ./frontend

      - name: Build frontend
        run: npm run build
        working-directory: ./frontend
        env:
          VITE_API_URL: ${{ env.API_GATEWAY_URL }}
          VITE_AUTH_MODE: cognito
          VITE_COGNITO_USER_POOL_ID: ${{ secrets.COGNITO_USER_POOL_ID }}
          VITE_COGNITO_CLIENT_ID: ${{ secrets.COGNITO_USER_POOL_CLIENT_ID }}
          VITE_AWS_REGION: ${{ env.AWS_REGION }}

      - name: Create runtime config.json
        run: |
          cat > ./frontend/dist/config.json <<EOF
          {
            "apiUrl": "${{ env.API_GATEWAY_URL }}",
            "authMode": "cognito",
            "cognitoUserPoolId": "${{ secrets.COGNITO_USER_POOL_ID }}",
            "cognitoClientId": "${{ secrets.COGNITO_USER_POOL_CLIENT_ID }}",
            "awsRegion": "${{ env.AWS_REGION }}"
          }
          EOF
          echo "âœ… Created config.json for runtime configuration"
          cat ./frontend/dist/config.json

      - name: Deploy to S3
        run: |
          aws s3 sync ./frontend/dist/ s3://${{ steps.discover-s3.outputs.bucket_name }} --delete

      - name: Get CloudFront Distribution ID
        id: get-cf-id
        run: |
          # Query by bucket name in origin instead of aliases (distributions without custom domains don't have aliases)
          BUCKET_NAME="${{ steps.discover-s3.outputs.bucket_name }}"
          echo "Looking for CloudFront distribution with origin: $BUCKET_NAME"

          DISTRIBUTION_ID=$(aws cloudfront list-distributions \
            --query "DistributionList.Items[?Origins.Items[?contains(DomainName, '$BUCKET_NAME')]].Id | [0]" \
            --output text)

          if [ -z "$DISTRIBUTION_ID" ] || [ "$DISTRIBUTION_ID" == "None" ]; then
            echo "âŒ ERROR: Could not find CloudFront distribution for bucket $BUCKET_NAME"
            exit 1
          fi

          echo "âœ… Found CloudFront distribution: $DISTRIBUTION_ID"
          echo "distribution_id=$DISTRIBUTION_ID" >> $GITHUB_OUTPUT

      - name: Invalidate CloudFront cache
        run: |
          aws cloudfront create-invalidation \
            --distribution-id ${{ steps.get-cf-id.outputs.distribution_id }} \
            --paths "/*"

  smoke-tests:
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, deploy-frontend]
    # Run even if frontend deployment was skipped
    if: always() && needs.deploy-infrastructure.result == 'success'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Test Identity Service health
        run: |
          echo "Testing Identity Service at ${{ needs.deploy-infrastructure.outputs.identity_service_url }}/health"
          curl -f ${{ needs.deploy-infrastructure.outputs.identity_service_url }}/health || exit 1

      - name: Test API Gateway health
        run: |
          echo "Testing API Gateway at ${{ needs.deploy-infrastructure.outputs.api_gateway_url }}/health"
          curl -f ${{ needs.deploy-infrastructure.outputs.api_gateway_url }}/health || exit 1

      - name: Test Data Service health
        run: |
          echo "Testing Data Service at ${{ needs.deploy-infrastructure.outputs.data_service_url }}/health"
          curl -f ${{ needs.deploy-infrastructure.outputs.data_service_url }}/health || exit 1

      - name: Test Frontend
        if: needs.deploy-infrastructure.outputs.s3_bucket != '' && needs.deploy-infrastructure.outputs.cloudfront_domain != ''
        run: |
          echo "Testing Frontend at https://${{ needs.deploy-infrastructure.outputs.cloudfront_domain }}"
          curl -f https://${{ needs.deploy-infrastructure.outputs.cloudfront_domain }} || exit 1

      - name: Frontend deployment was skipped
        if: needs.deploy-infrastructure.outputs.s3_bucket == '' || needs.deploy-infrastructure.outputs.cloudfront_domain == ''
        run: |
          echo "â­ï¸ Frontend deployment was skipped (S3 bucket or CloudFront not available)"
          echo "This is normal if this is a partial deployment."

  notify:
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, deploy-frontend, smoke-tests]
    if: always()
    env:
      AWS_REGION: us-east-1

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get deployment URLs
        id: get-urls
        run: |
          # Get S3 bucket
          S3_BUCKET=$(aws s3api list-buckets --query "Buckets[?contains(Name, '-dev-frontend')].Name | [0]" --output text)

          # Get CloudFront distribution domain
          CF_DOMAIN=$(aws cloudfront list-distributions \
            --query "DistributionList.Items[?Origins.Items[?contains(DomainName, '$S3_BUCKET')]].DomainName | [0]" \
            --output text)

          # Get API Gateway URL (from App Runner)
          API_GW=$(aws apprunner list-services --query "ServiceSummaryList[?contains(ServiceName, '-dev-api-gateway')].ServiceUrl | [0]" --output text)

          echo "cloudfront_url=https://$CF_DOMAIN" >> $GITHUB_OUTPUT
          echo "api_gateway_url=https://$API_GW" >> $GITHUB_OUTPUT

      - name: Notify deployment status
        run: |
          # Check if frontend was deployed (even if smoke-tests were skipped)
          FRONTEND_DEPLOYED="${{ needs.deploy-frontend.result }}"
          INFRA_DEPLOYED="${{ needs.deploy-infrastructure.result }}"
          SMOKE_RESULT="${{ needs.smoke-tests.result }}"

          if [ "$FRONTEND_DEPLOYED" == "success" ]; then
            echo "âœ… Frontend deployment successful!"
            echo ""
            echo "ğŸŒ Application URLs:"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "Frontend:    ${{ steps.get-urls.outputs.cloudfront_url }}"
            if [ "$INFRA_DEPLOYED" == "success" ] || [ "$SMOKE_RESULT" == "success" ]; then
              echo "API Gateway: ${{ steps.get-urls.outputs.api_gateway_url }}"
            fi
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo ""
            echo "ğŸ’¡ Test your application:"
            echo "   Open: ${{ steps.get-urls.outputs.cloudfront_url }}"

            # Exit with success if frontend deployed successfully
            exit 0
          elif [ "$SMOKE_RESULT" == "success" ]; then
            echo "âœ… Dev deployment successful!"
            echo ""
            echo "ğŸŒ Application URLs:"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "Frontend:    ${{ steps.get-urls.outputs.cloudfront_url }}"
            echo "API Gateway: ${{ steps.get-urls.outputs.api_gateway_url }}"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo ""
            echo "ğŸ’¡ Test your application:"
            echo "   Open: ${{ steps.get-urls.outputs.cloudfront_url }}"
            exit 0
          elif [ "$SMOKE_RESULT" == "skipped" ] && [ "$FRONTEND_DEPLOYED" == "skipped" ]; then
            echo "â­ï¸ Deployment steps skipped (no changes detected)."
            echo "âœ… Treating as successful run for notify."
            exit 0
          else
            echo "âŒ Dev deployment failed!"
            echo "Check the logs above for details."
            if [ "$FRONTEND_DEPLOYED" == "failure" ]; then
              echo "   - Frontend deployment failed"
            fi
            if [ "$INFRA_DEPLOYED" == "failure" ]; then
              echo "   - Infrastructure deployment failed"
            fi
            if [ "$SMOKE_RESULT" == "failure" ]; then
              echo "   - Smoke tests failed"
            fi
            exit 1
          fi
